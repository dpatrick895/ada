{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Assignment 1 : Literature review - Daniel Patrick\n",
    "\n",
    "Here is my Jupyter Notebook on Github for a better viewing experience:<br>\n",
    "https://github.com/dpatrick895/ada/blob/master/A1/Literature%20Review.ipynb\n",
    "\n",
    "In the PDF version I have removed the inline reference links as it was causing difficulties with the Jupyter Notebook to PDF conversion. The links are embedded within the document on github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. My First Read\n",
    "\n",
    "Here I will outline what I encountered in my first read of *Opinion mining with Deep Recurrent Neural Networks* [(Irsoy and Cardie, 2014)](https://www.cs.cornell.edu/~oirsoy/files/emnlp14drnt.pdf). I am reading this paper with the following goals in mind.\n",
    "-  Investigate the field of study known as opinion mining \n",
    "-  Investigate the usefulness of using a recurrent neural network to perform opinion mining tasks\n",
    "- Further my understanding of neural networks by reading this paper and other resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Recurrent Neural Networks\n",
    "\n",
    "- What is a Recurrent Neural Network (RNN)?\n",
    "- Why are they useful when it comes to opinion mining?\n",
    "- How does the training process for RNN work?\n",
    "- I want to see an example of a RNN in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Read and Write Critical Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Content\n",
    "\n",
    "This paper is about Opinion Mining, \"Fine-grained opinion analysis aims to detect subjective expressions in a text (e.g. \"hate\") and to characterize their intensity (e.g. strong) and sentiment (e.g. negative)\" [(Irsoy and Cardie, 2014)](https://www.cs.cornell.edu/~oirsoy/files/emnlp14drnt.pdf). Opinion mining is related to areas of computer science such as natural language processing, text analysis and computational linguistics. The goal of opinion mining, is to take an input sentence and then output information describing the subjective expressions, intensity and sentiment.\n",
    "The authors achieve this by feeding a sentences into machine learning algorithms in an attempt to classify the sentences into either *direct subjective expressions* (DSEs) or *expressive subjective expressions* (ESEs) [(Wiebe et al., 2005)](https://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf).\n",
    "\"DSE's consist of private states or speech events expressing private states; and ESE's consist of expressions that indicate sentiment, emotion, etc., without explicitly conveying them\" [(Irsoy and Cardie, 2014)](https://www.cs.cornell.edu/~oirsoy/files/emnlp14drnt.pdf). This concept of DSEs ans ESEs become clearer when you have a look at the example in the paper which is included in the notebook below. In the past, opinion mining problems have been attempted to be solved as sequence labeling problems for example in the work of [(Choi et al., 2005)](https://dl.acm.org/citation.cfm?id=1220620). To utilize this research that has been undertaken in the past Isroy and Cardie used the BIO tagging scheme. The BIO tagging scheme is a simple tagging system where the 'B' indicates the beginning of the of an opinion related expression, 'I' corresponds to all of the tokens inside the opinion related expression, and the 'O' indicates tokens outside any opinion related class. Below is the example provided in the paper:  \n",
    "\n",
    "<img  src=\"table1.PNG\">\n",
    "\n",
    "\n",
    "On inspection of this sentence in the figure above it is clear that there is expression and tone with the \"as usual\" and \"refused to make any statements\".\n",
    "\n",
    "When I analyse this sentence, I do see that the \"as usual\" is expressing the opinion of the writer. To me it seems like the writer is being slightly sarcastic but also seems a little bit exacerbated. You can begin to see how there is a lot of value in being able to detect these ESEs and DSEs as it would allow people to gather people's opinion at an industrial scale. The application of this being quite large, for example being able to measure the opinion and sentiment in the public about a certain topic on twitter or facebook for example.\n",
    "\n",
    "When it comes to selecting a specific machine learning approach for the goal of classifying the subjective expression, intensity and sentiment. It can depend on the way in which the problem is viewed. For example in the paper *Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns* [(Choi et al., 2005)](https://dl.acm.org/citation.cfm?id=1220620) they stated “We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields [(Lafferty et at., 2001)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers) and a variation of AutoSlog [(Riloff, 1996a)](https://www.cs.utah.edu/~riloff/pdfs/aaai96.pdf)”. This method was one of the approaches which was compared against the Recurrent Neural Network in the paper. Below is a short summary of each algorithm referenced in the paper. It is also valuable to note that, when this paper was established they were comparing the RNN approach to the current state of the art algorithms. These algorithms are listed below:\n",
    "\n",
    "    \n",
    "### Conditional Random Field (CRF)\n",
    "The conditional random field was one of the earlier attempts at a machine learning algorithm applied to Opinion Mining, Lafferty describes the CRF as “a framework for building probabilistic models to segment and label sequence data”\n",
    "[(Lafferty et at., 2001)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)\n",
    "CRFs are a type of discriminative undirected probabilistic graphical model. It is used to encode relationships between observations and construct consistent interpretations. \n",
    "\n",
    "    \n",
    "### AutoSlog\n",
    "In Choi’s paper they combined the CRF algorithm with AutoSlog an algorithm developed by Riloff, Riloff says in her paper “We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text”. <br> \n",
    "After combining these two algorithms to make an ensemble classifier  [(Riloff, 1996)](https://www.cs.utah.edu/~riloff/pdfs/aaai96.pdf) went on to say “While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns”. Choi then says that this ensemble classifier is more accurate than the two methods performing by themselves. This ensemble method was the algorithm that was compared against the Recurrent Neural Networks discussed in this paper. \n",
    "\n",
    "### SemiCRF \n",
    "SemiCRF is the last algorithm other than the Recurrent Neural Network which was used to compare against the Recurrent Neural Network, in Isroy and Cardie’s paper. “We describe semi-Markov conditional random fields (semi-CRFs), a conditionally trained version of semi-Markov chains.” [(Sarawagi and Cohen, 2004)](http://www.cs.cmu.edu/~wcohen/postscript/semiCRF.pdf). This approach was also used to compare against the Recurrent Neural Network as it had been experimentally shown that semiCRF’s were out performing traditional CRF’s. \n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "The Recurrent Neural Network was the focus of this paper for the Opinion Mining so in this section of the review I speak about what a Recurrent Neural Network is. A Recurrent Neural Network is an extended version of a simpler Artificial Neural Network. <br>\n",
    "In the book Discovering Knowledge in Data an Artificial Neural Network is described as “an attempt at a very basic level to imitate the type of non-linear learning that occurs in the networks of neurons found in nature” [(Larose & Larose, 2014)](https://onlinelibrary.wiley.com/doi/book/10.1002/0471687545). This description of what a neural network is, gives me a basic understanding of why we would want to apply this type of algorithm to a task such as Opinion Mining as it is a problem which is nonlinear in nature.\n",
    "\n",
    "More specifically, a RNN is a artificial neural network where connections between nodes form a direct graph along a sequence. Interestingly unlike basic artificial neural networks RNNs can use their internal state otherwise known as 'memory' to process sequences of inputs. This is very useful for a task like opinion mining, because the opinion contained within a sentence or paragraph is made up of a sequence of words. It makes a lot of sense that the authors selected an RNN to approach this problem, given the inherent structure and functionality of the recurrent neural network itself.\n",
    "\n",
    "Below are the recurrent neural network architecture diagrams from within the paper itself.\n",
    "\n",
    "<img  src=\"figure1.PNG\">\n",
    "\n",
    "\n",
    "### Suggestions for Future Work\n",
    "\n",
    "In the results and discussion section of the paper the authors said “In general, CRFs exhibit high precision but low recall, while semiCRFs exhibit a high recall, low precision performance. Compared to semiCRF the deep RNNs produce an even higher recall but sometimes lower precision for ESE detection. This suggests that the methods are complementary, and can be potentially be even more powerful when combined in an ensemble method” (Isroy and Cardie, 2014). So considering that we see that the contemporary algorithms for opinion mining at the time were the CRF or semiCRF. If they were combined with the deep RNN, they could potentially produce even greater result as an ensemble method. \n",
    "\n",
    "With that in mind that, this would probably be the direction for future study. However first wer would want to conduct a meta-analysis on current opinion mining algorithms then attempt to create an ensemble classifier that could potentially produce greater accuracy.\n",
    "\n",
    "As we saw with Liu et al., 2015 they also used RNNs for fine grained opinion analysis however they used a different RNN architecture. So combining the current state of the art RNN algorithm with the best CRF algorithm utilizing word vectors would be very interesting research.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "To summarize all of the information above, this paper has good content because it contains good information. As I have discussed above the paper is focused on opinion mining which is in the domain of text analysis, sentiment analysis and natural language processing. <br>\n",
    "The name of the paper definitely corresponds to the content of the paper. The paper is called \"Opinion mining with recurrent neural networks\" and the paper discusses approaches to opinion mining as well as how they used recurrent neural networks to solve this problem.<br>\n",
    "Additionally the paper discusses and compares several machine and statistical learning algorithms against the algorithm that they implemented being the recurrent neural network. The other algorithms discussed include:\n",
    "- CRFs \n",
    "- AutoSlog\n",
    "- Semi-CRF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Innovation\n",
    "\n",
    "Considering this paper was published in 2014, there is a little bit of retrospective review that needs to take place. I would argue that although this paper may not currently be 100% cutting edge now, it did definitely help pave the way for the use of Recurrent Neural Networks being applied to Natural Language Processing and Sentiment / Opinion mining tasks.<br>\n",
    "This paper achieved this by showing the comparison of the current state of the art approaches, which demonstrated that Recurrent Neural Networks tend to perform better than the other contemporary algorithms.\n",
    "\n",
    "When considering the background of this paper, I would consider the authors. Based on my research the main author of this paper Claire Cardie has been involved in opinion mining and computational linguistics for a long time. So the background of this paper is essentially researching ways in which they can make a model which is more effective at performing opinion mining tasks.\n",
    "\n",
    "This paper is novel because it is one of the first attempts at applying Recurrent Neural Networks to the task of Opinion Mining. Using RNNs the authors found that they were able to achieve better results than the current state of the art algorithms at the time. Additionally it should be noted that the CRF and the semiCRF's had access to \"word vectors\". The RNN did not use the word vectors, this means that the CRFs and semiCRFs had access to more information. In a sense the CRFs and semiCRFs had to be provided more information i.e the word vectors to achieve an accurate prediction. Whereas the Recurrent Neural Network was able to 'learn' the patterns and as such did not require the support of the word vectors. The word vectors are 300-dimensional data points [(Mikilov et al, 2013)](https://arxiv.org/pdf/1301.3781.pdf). These word vectors were trained on the google news dataset which is around 100 billion words.\n",
    "\n",
    "    \n",
    "Considering the fact that the RNNs outperformed the CRFs and semiCRFs without using the word vectors indicates that using RNNs is a  good approach for conducting Opinion Mining tasks. This paper has been well received in the papers that have cited it and it has been the foundation for the paper \"Fine-grained opinion analysis with Recurrent Neural Networks and Word Embeddings\" [(Liu et al., 2015)](http://www.aclweb.org/anthology/D15-1168). Liu states in their paper that Isroy and Cardie's work was the foundation of their paper. However they used a different RNN architecture. Liu used Jordan and LSTM RNNs whereas Isroy and Cardie used Elman type RNNs.\n",
    "\n",
    "    \n",
    "There is also other work that has cited this paper that is not in the realm of Natural Language Processing. Soggard and Goldberg's 2016 [paper](http://anthology.aclweb.org/P16-2038) is looking at deep multi-task learning and they cite Isroy and Cardie's paper for their contribution to developing a sequence tagging architecture.<br>\n",
    "We can see that this work was novel and innovative it was one of the first implementations of using RNNs for opinion mining which other people have now utilized in their work. In addition to this I would argue that Isroy and Cardie's paper was novel and innovative because it achieved good results and paved the way for future research as well as forming the foundation for several other papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Technical Quality\n",
    "\n",
    "Technically speaking I believe this paper is of high quality, it was authored by a distinguished scholar Claire Cardie with over 16,000 citations (Google Scholar, 2018). To me that indicates that the author is a respected academic in the community. However we should not judge the paper based on who wrote it, but rather the content of the paper itself. Having read the paper I believe that their research methodology is sound. In the paper Isroy and Cardie gave an explanation of their methodology which I will explain below. Providing a brief explanation of certain terms that may not be obvious to a non expert of the subject area. <br>\n",
    "Firstly they trained 6 recurrent neural networks, with one being a shallow recurrent neural network and the other five being deep recurrent neural networks they said \"We employ the standard softmax activation for the output layer: $g(x) = e^{x_i} / \\Sigma{_j}e^{x_j}$. \n",
    "For the hidden layers we use the rectifier linear activation: $f(x) = max\\left\\{0, x\\right\\}$ Experimentally, rectifier activation gives better performance, faster convergence, and sparse representations.\" [(Irsoy and Cardie, 2014)](https://www.cs.cornell.edu/~oirsoy/files/emnlp14drnt.pdf)<br>\n",
    "I see this to be an advantage because they are comparing several types of network architecture to determine which network is the most effective for this task.\n",
    "\n",
    "Interestingly enough, when I was trying out an RNN below in section 3. I found that the 'sigmoid' activator performed better than the 'relu' which is rectifier linear activation as opposed to the sigmoid activation that I used in section 3. Perhaps this is attributed to their implementation being different to mine.\n",
    "\n",
    "In addition to this they specify the dataset that they are using which is the \"MPQA 1.2 corpus [(Wiebe et al., 2005)](https://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf)\". This dataset contains 535 news articles which contains 11,111 sentences. These sentences were manually annotated with the DSEs and ESEs so that they can test if their algorithm produced the correct classification of the sentence. The fact that they have explained the dataset they have used and discussed how they have designed and trained the network indicates that the paper is of high quality.\n",
    "\n",
    "    \n",
    "Isroy and Cardie then discussed how they trained their networks saying that \"We use the standard multiclass cross-entropy as the objective function when training the networks” [(Irsoy and Cardie, 2014)](https://www.cs.cornell.edu/~oirsoy/files/emnlp14drnt.pdf). We note that the ‘objective function’ is the loss function, which as we know is how the neural network adjusts the weights of the perceptrons by looking to minimise the loss function.<br>\n",
    "They then say “We use stochastic gradient descent with with a fixed learning rate (.005) and a fixed momentum rate (.7). We update weights after mini batches of 80 sentences. We run 200 epochs for training”.\n",
    "Basically defining these terms we note that, Stochastic gradient descent is a way to approximate the gradient descent because calculating many derivatives can be computationally complex. So they use stochastic approximation which is essentially a recursive approximation in this case the gradient of the loss function. Having seen how long it took to train my RNN, which took about 5 minutes per epoch, I can see why they looked to use stochastic gradient decent over normal gradient decent. If they did not do this it would have made the training process a lot more cumbersome. In my opinion the fact that they understood this and then optimized their Machine Learning pipeline to me this indicates that their approach is of high technical quality.\n",
    "\n",
    "    \n",
    "When they speak about the ‘epochs’ they are referring to one whole pass through the network using all of the training data. Once all of the training data is propagated through the network, then back propagated after this process has happened one epoch has occurred and as they said they run the network for 200 epochs.<br>\n",
    "\n",
    "They then presented their data on their comparison between the semiCRF and the deep recurrent neural network, as well as providing a table with the experimental results. After this there was a discussion about CRF vs RNN and as a result they came to the conclusion that both approaches have strengths and weaknesses and could potentially be combined to produce a better classification algorithm ensemble.  \n",
    "Considering all of these factors I believe that this makes the paper of high technical quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4  Application and X-Factor\n",
    "\n",
    "When we consider that the goal of opinion mining / sentiment analysis is to create algorithms or methodologies that can identify, extract and quantify subjective information and study affective states. It is clear that there is application for this in many areas. In our current Social Media world there is so much posted to the internet. With the use of an opinion mining tool if it were to be created, a company could determine what are high priority posts on their page. For example if a customer is upset or angry as determined by an opinion mining algorithm. Then the system could direct an employee to this post in an attempt to resolve the situation. Conversely the system could also report on positive feedback. This is only one simple use case where opinion mining could be used in industry. However this industrial application is an 'X-Factor' to me.\n",
    "\n",
    "\n",
    "There are also many academic thoughts that come to mind when considering opinion mining. If there were algorithms created that could produce high classification accuracy then we could study multiple pieces of text to ask questions like. What makes a piece of writing good? Or when we look at the best speeches of all time what level of opinions were expressed. Another idea could be potentially creating algorithms that can write texts based on certain input parameters.  These are just potential ideas and the ability to implement these ideas could potentially be too difficult however it is interesting to think about them nonetheless.\n",
    "\n",
    "I do believe that the application domain is appropriate for the technique described. Because unlike feedforward neural networks, RNNs can use their internal state to process sequences of inputs. This means that RNNs are suited to tasks such as handwriting recognition or speech recognition. As we know opinion mining is a natural language processing task. So therefore the application of an RNN for an opinion mining is a good idea because the algorithm is suited to a problem like this.\n",
    "\n",
    "Following on from that, the main application domains that RNNs can be applied to are:\n",
    "\n",
    "- Natural language processing\n",
    "- Speech recognition \n",
    "- Text recognition \n",
    "- Sentiment analysis\n",
    "\n",
    "There is also an interesting paper [here](https://cs.stanford.edu/people/karpathy/deepimagesent/) that combines Convolutional Neural Networks with RNNs to perform image recognition tasks but then to also label the image that has been detected. To me that is a very interesting project and I will take a look though the code when I get a chance.\n",
    "\n",
    "Furthermore the fact that we have got to a stage where people can develop and utilize algorithms that can accurately determine subjective opinion in text is quite an achievement. Considering we are in a machine learning / data analytics class. I think this is a topic that once explained the whole class would find the study area of opinion mining  interesting. Before this assignment I did not realise the scale and scope of this area of computer science so that is another ‘x-factor’ to me. Getting exposure to these things such as:<br>\n",
    "Opinion Mining, Sentiment Analysis, Natural Language Processing and Text Mining was definitely very interesting experience and was an x-factor moment for me during my reading of the paper.\n",
    "    \n",
    "Another aspect that I found interesting about the work was that using the deep RNN they were able to produce results that were very good if not the best at the time. Considering that the other state of the art algorithms and approaches utilized word vectors [(Mikilov et al, 2013)](https://arxiv.org/pdf/1301.3781.pdf). The fact that this new approach is very competitive against the other state of the art algorithms and does not use word vectors at all is huge and an 'X-factor' achievement of this paper and speaks to the usefulness of applying RNNs to the task of sentiment analysis and opinion mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Presentation \n",
    "\n",
    "I think that the presentation of this paper is of high quality. The authors clearly explained the problem of opinion mining and why it is important to develop even better algorithms to attempt to solve these problems. They explained why they wanted to attempt these problems using a Recurrent Neural Network approach.\n",
    "They had a theory that Deep RNNs would have the ability to outperform the current state of the art algorithms, after investigating this claim they were able to experimentally verify this.\n",
    "\n",
    "The way in which the paper was laid out made it quite easy to follow. When it came to the Recurrent neural network they used they explained what dataset they trained it on and how they trained it. The paper was presented in a way such that a person who has knowledge in the field of machine learning could also implement a RNN with the same structure as they provided. A more detailed explanation of how they set up the RNN is provided in the technical quality section of this paper.  \n",
    "\n",
    "I think that the authors arguments were concisely constructed and easy to follow. So therefore I believe that this paper was presented clearly and to a high standard for the time in which it was published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Try the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All credit [here](https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e) goes to Susan Li from her article *A Beginner's Guide on Sentiment Analysis with RNN*. As a result of playing around with her code I have seen how easy it is to set up an RNN for sentiment analysis. Specifically when using the Tensorflow and Keras packages.\n",
    "\n",
    "I think that for a basic implementation the accuracy achieved is quite good. I would like to build upon this knowledge for further study within opinion mining and sentiment analysis.\n",
    "\n",
    "You can see that with a simple implementation I was able to achieve a 87.5% accuracy on the test data set, this demonstrates how simple it is to set up a quick implementation of a RNN utilizing the Keras and Tensorflow libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded datset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.preprocessing import sequence\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "24936/24936 [==============================] - 167s 7ms/step - loss: 0.4831 - acc: 0.7634 - val_loss: 0.2795 - val_acc: 0.8750\n",
      "Epoch 2/3\n",
      "24936/24936 [==============================] - 178s 7ms/step - loss: 0.3457 - acc: 0.8546 - val_loss: 0.2842 - val_acc: 0.9062\n",
      "Epoch 3/3\n",
      "24936/24936 [==============================] - 176s 7ms/step - loss: 0.2704 - acc: 0.8957 - val_loss: 0.2399 - val_acc: 0.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2273683f0b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.87524\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
