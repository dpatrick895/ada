{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA assignment 1 : Literature review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. My First Read\n",
    "\n",
    "Here I will outline what I encountered in my first read of *Opinion mining with Deep Recurrent Neural Networks* (Irsoy and Cardie, 2014). I am reading this paper with the following goals in mind.\n",
    "-  Investigate the field of study known as opinion mining \n",
    "-  Investigate the usefulness of using a recurrent neural network to perform opinion mining tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Read and Write Critical Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Content\n",
    "\n",
    "This paper is about Opinion Mining \"Fine-grained opinion analysis aims to detect subjective expressions in a text (e.g. \"hate\") and to characterize their intensity (e.g. strong) and sentiment (e.g. negative)\" (Irsoy and Cardie, 2014). Because of this we can think of sentiment analysis as a subset of opinion mining. Opinion mining is related to areas of computer science such as natural language processing, text analysis and computational linguistics. The goal of opinion mining, is to take an input sentence then output information describing the subjective expressions, intensity and sentiment.\n",
    "The authors achieve this by feeding a sentences into machine learning algorithms in an attempt to classify the sentences into either *direct subjective expressions* (DSEs) or *expressive subjective expressions* (ESEs) (Wiebe et al., 2005).\n",
    "\"DSE's consist of private states or speech events expressing private states; and ESE's consist of expressions that indicate sentiment, emotion, etc., without explicitly conveying them\" (Isroy and Cardie, 2014). In the past, opinion mining problems have been attempted to be solved as sequence labeling problems for example in the work of (Choi et al., 2005). To utilize research that has been undertaken in the past Isroy and Cardie used the BIO tagging scheme. The BIO tagging scheme is a simple tagging system where the B indicates the beginning of the of an opinion related expression, I corresponds to all of the tokens inside the opinion related expression, and the O indicates tokens outside any opinion related class. Below is the example provided in the paper:  \n",
    "<!-- This is how to make comments in Markdown style=\"float:left;\"-->\n",
    "<img  src=\"table1.PNG\">\n",
    "\n",
    "\n",
    "On inspection of this sentence in the figure above it is clear that there is expression and tone with the \"as usual\" and \"refused to make any statements\"\n",
    "<!-- Do a little more on the explanation on the tagging scheme -->\n",
    "When it comes to selecting a specific machine learning approach for the goal of classifying the subjective expression, intensity and sentiment. It can depend on the way in which the problem is viewed. For example in the paper *Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns* (Choi et al., 2005) they stated “We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et at., 2001) and a variation of AutoSlog (Riloff, 1996a)”. This method was one of the approaches which was compared against the Recurrent Neural Network in the paper. Below is a short summary of each algorithm referenced in the paper.\n",
    "\n",
    "    \n",
    "### Conditional Random Field (CRF)\n",
    "The conditional random field was one of the earlier attempts at a machine learning algorithm applied to Opinion Mining, Lafferty describes the CRF as “a framework for building probabilistic models to segment and label sequence data”\n",
    "<mark>CITATION REQUIRED </mark>\n",
    "CRFs are a type of discriminative undirected probabilistic graphical model. It is used to encode relationships between observations and construct consistent interpretations. \n",
    "\n",
    "    \n",
    "### AutoSlog\n",
    "In Choi’s paper they combined the CRF algorithm with AutoSlog an algorithm developed by Riloff, Riloff says in her paper “We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text”. <br> \n",
    "After combining these two algorithms to make an ensemble classifier  (Choi et al., 2005) went on to say “While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns”. Choi then says that this ensemble classifier is more accurate than the two methods performing by themselves. This ensemble method was the algorithm that was compared against the Recurrent Neural Networks discussed in this paper.\n",
    "\n",
    "### SemiCRF \n",
    "SemiCRF is the last algorithm other than the Recurrent Neural Network which was used to compare against the Recurrent Neural Network, in Isroy and Cardie’s paper. “We describe semi-Markov conditional random fields (semi-CRFs), a conditionally trained version of semi-Markov chains.” (Sarawagi and Cohen, 2004). This approach was also used to compare against the Recurrent Neural Network as it had been experimentally shown that semiCRF’s were out performing traditional CRF’s. \n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "The Recurrent Neural Network was the focus of this paper for the Opinion Mining so in this section of the review I speak about what a Recurrent Neural Network is. A Recurrent Neural Network is an extended version of a simpler Artificial Neural Network. <br>\n",
    "In the book Discovering Knowledge in Data a Artificial Neural Network is described as “an attempt at a very basic level to imitate the type of non-linear learning that occurs in the networks of neurons found in nature” (Larose & Larose, 2014). This description of what a neural network is, gives me a basic understanding of why we would want to apply this type of algorithm to a task such as Opinion Mining as it is a problem which is nonlinear in nature.\n",
    "\n",
    "### Summary\n",
    "\n",
    "To summarize all of the information above, this paper has good content because it satisfies the requirements of the assignment specification. As I have discussed above the paper is focused on opinion mining which is in the domain of, text analysis, sentiment analysis and natural language processing. <br>\n",
    "The name of the paper definitely corresponds to the content of the paper. The paper is called \"Opinion mining with recurrent neural networks\" and the paper discusses approaches to opinion mining as well as how they used recurrent neural networks to solve this problem. This also makes the content of this paper high quality.<br>\n",
    "Additionally as I have discussed above the paper discusses and compares several machine and statistical learning algorithms against the algorithm that they implemented being the recurrent neural network. The other algorithms discusses include:\n",
    "- CRFs \n",
    "- AutoSlog\n",
    "- Semi-CRF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"https://media.giphy.com/media/K25wpPoaYmo9y/giphy.gif\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Innovation\n",
    "\n",
    "This paper is novel because it is one of the first attempts at applying Recurrent Neural Networks to the task of Opinion Mining. Using RNNs the authors found that they were able to achieve better results than the current state of the art algorithms at the time. Additionally it should be noted that the CRF and the semiCRF's had access to \"word vectors\". The RNN did not use the word vectors, this means that the CRFs and semiCRFs had access to more information. The word vectors are 300-dimensional data points (Mikilov et al, 2013). These word vectors were trained on the google news dataset which is around 100 billion words.\n",
    "\n",
    "    \n",
    "Considering the fact that the RNNs outperformed the CRFs and semiCRFs without using the word vectors indicates that using RNNs is a very good approach for conducting Opinion Mining tasks. This paper has been well received in the papers that have sited it and it has been the foundation for the paper \"Fine-grained opinion analysis with Recurrent Neural Networks and Word Embeddings\" (Liu et al., 2015). Liu states in their paper that Isroy and Cardie's work was the foundation of their paper. However they used a different RNN architecture. Liu used Jordan and LSTM RNNs whereas Isroy and Cardie used Elman type RNNs.\n",
    "\n",
    "    \n",
    "There is also other work that has cited this paper that is not in the realm of Natural Language Processing. Soggard and Goldberg's 2016 paper is looking at deep multi-task learning and they cite Isroy and Cardie's paper for their contribution to developing a sequence tagging architecture.<br>\n",
    "We can see that this work was novel and innovative it was one of the first implementations of using RNNs for opinion mining which other people have now utilized in their work. I would argue that Isroy and Cardie's paper was novel and innovative that achieved good results and paved the way for future research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Technical Quality\n",
    "\n",
    "Technically speaking I believe this paper is of high quality, it was authored by a distinguished scholar Claire Cardie with over 16,000 citations (Google Scholar, 2018). To me that indicates that the author is a respected academic in the community. However we should not judge the paper based on who wrote it, but rather the content of the paper itself. Having read the paper I believe that their research methodology is sound. In the paper Isroy and Cardie gave an explanation of their methodology which I will explain below. Giving a brief explanation of certain terms that may not be obvious to a non expert of the subject area. <br>\n",
    "<mark>Firstly they trained 6 recurrent neural networks, with one being a shallow recurrent neural network and the other five being deep recurrent neural networks they said \"We employ the standard softmax activation for the output layer: $g(x) = e^{x_i}$ <br>\n",
    "For the hidden layers we use the rectifier linear activation: $f(x) = max\\left\\{0, x\\right\\}$ Experimentally, rectifier activation gives better performance, faster convergence, and sparse representations.\" (Isroy and Cardie, 2014) <mark><br>\n",
    "In addition to this they specify the dataset that they are using which is the \"MPQA 1.2 corpus (Wiebe et al., 2005)\". This dataset contains 535 news articles which contains 11,111 sentences. These sentences were manually annotated with the DSEs and ESEs so that they can test if their algorithm produced the correct classification of the sentence. \n",
    "\n",
    "    \n",
    "Isroy and Cardie then discussed how they trained their networks saying that \"We use the standard multiclass cross-entropy as the objective function when training the networks” (Isroy and Cardie, 2014). We note that the ‘objective function’ is the loss function, which as we know is how the neural network adjusts the weights of the perceptrons by looking to minimise the loss function.<br>\n",
    "They then say “We use stochastic gradient descent with with a fixed learning rate (.005) and a fixed momentum rate (.7). We update weights after mini batches of 80 sentences. We run 200 epochs for training”.\n",
    "Basically defining these terms we note that, Stochastic gradient descent is a way to approximate the gradient descent because calculating many derivatives can be computationally complex. So they use stochastic approximation which is essentially a recursive approximation in this case the gradient of the loss function.\n",
    "\n",
    "    \n",
    "When they speak about the ‘epochs’ they are referring to one whole pass through the network. So the data is propagated through the network, then back propagated after this process has happened one epoch has occurred and as they said they run the network for 200 epochs.<br>\n",
    "They then presented their data on their comparison between the semiCRF and the deep recurrent neural network, as well as providing a table with the experimental results. After this there was a discussion about CRF vs RNN and as a result they came to the conclusion that both approaches have strengths and weaknesses and could potentially be combined to produce a better classification algorithm ensemble.  \n",
    "Considering all of these factors I believe that this makes the paper of high technical quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 X-Factor\n",
    "\n",
    "I think that this paper could definitely spark a good discussion in class for the following reasons:\n",
    "\n",
    "I think that primarily it is an interesting topic, the fact that we have got to a stage where people can develop and utilize algorithms that can accurately determine subjective opinion in text is quite an achievement. Considering we are in a machine learning / data analytics class. I think this is a topic that once explained the whole class would find the study area of opinion mining very interesting. Furthermore before this assignment I did not realise the scale and scope of this area of computer science so that is another ‘x-factor’ to me. Getting exposure to these things such as:<br>\n",
    "Opinion Mining, Sentiment Analysis, Natural Language Processing and Text Mining was definitely very interesting  experience and was the  x-factor moment for me during my reading of the paper.\n",
    "\n",
    "    \n",
    "Another aspect that I found interesting about the work was that using the deep RNN they were able to produce results that were very good if not the best. Considering that the other state of the art algorithms and approaches utilize word vectors Mikolov et al. (2013). The fact that this new approach is very competitive against the other state of the art algorithms and does not use a word vector as well is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Presentation \n",
    "\n",
    "I think that the presentation of this paper is of high quality. The authors clearly explained the problem of opinion mining and why it is important to develop even better algorithms to attempt to solve these problems. They explained why they wanted to attempt these problems using a Recurrent Neural Network approach.\n",
    "\n",
    "The way in which the paper was laid out made it quite easy to follow. When it came to the Recurrent neural network they used they explained what dataset they trained it on and how they trained it. The paper was presented in a way such that a person who has knowledge in the field of machine learning could also implement a RNN with the same structure as they provided. A more detailed explanation of how they set up the RNN is provided in the technical quality section of this paper.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Application\n",
    "\n",
    "When we consider that the goal of opinion mining / sentiment analysis is to create algorithms or methodologies that can identify, extract and quantify subjective information and study affective states. It is clear that there is application in many areas. In our current Social Media world there is so much posted to the internet. With the use of an opinion mining tool if it were to be created, a company could determine what are high priority posts on their page. For example if a customer is upset or angry as determined by an opinion mining algorithm. Then the system could direct an employee to this post in an attempt to resolve the situation.\n",
    "Conversely the system could also report on positive feedback. \n",
    "\n",
    "\n",
    "There are also many academic thoughts that come to mind when considering opinion mining. If there were algorithms created that could produce high classification accuracy then we could study multiple pieces of text to ask questions like. What makes a piece of writing good? Or when we look at the best speeches of all time what level of opinions were expressed. Another idea could be potentially creating algorithms that can write texts based on certain input parameters.  These are just potential ideas and the ability to implement these ideas could potentially be too difficult however it is interesting to think about them nonetheless.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Future Work\n",
    "\n",
    "In the results and discussion section of the paper the authors said “In general, CRFs exhibit high precision but low recall, while semiCRFs exhibit a high recall, low precision performance. Compared to semiCRF the deep RNNs produce an even higher recall but sometimes lower precision for ESE detection. This suggests that the methods are complementary, and can be potentially be even more powerful when combined in an ensemble method” (Isroy and Cardie, 2014). So considering that we see that the contemporary algorithms for opinion mining at the time were the CRF or semiCRF. If they were combined with the deep RNN, they could potentially produce even greater result as an ensemble method. \n",
    "\n",
    "With that in mind that, this would probably be the direction for future study. However first wer would want to conduct a meta-analysis on current opinion mining algorithms then attempt to create an ensemble classifier that could potentially produce greater accuracy.\n",
    "\n",
    "As we saw with Liu et al., 2015 they also used RNNs for fine grained opinion analysis however they used a different RNN architecture. So combining the current state of the art RNN algorithm with the best CRF algorithm utilizing word vectors would be very interesting research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Try the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Patrick\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded datset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "Label:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('Review:')\n",
    "print(X_train[0])\n",
    "print('Label:')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review with words---\n",
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.preprocessing import sequence\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "24936/24936 [==============================] - 248s 10ms/step - loss: 0.5195 - acc: 0.7330 - val_loss: 0.3296 - val_acc: 0.8438\n",
      "Epoch 2/3\n",
      "24936/24936 [==============================] - 253s 10ms/step - loss: 0.2993 - acc: 0.8777 - val_loss: 0.1980 - val_acc: 0.9375\n",
      "Epoch 3/3\n",
      "24936/24936 [==============================] - 260s 10ms/step - loss: 0.2330 - acc: 0.9094 - val_loss: 0.1869 - val_acc: 0.9531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1736f9d1e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
