{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Reading Notes\n",
    "\n",
    "This is an example of noting your own reading, understanding, and realising process of an academic paper [[Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661)]. \n",
    "\n",
    "__NB1__: Critical commnents, as required by Assignment 1, are NOT included (Section 2 is a placeholder). \n",
    "\n",
    "__NB2__: The contents in the example show an active learning experience, so not all arguments are accurate and consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. My First Read\n",
    "\n",
    "You can freely note all your confusions and your effort on understanding:\n",
    "- unfamilier notions, \n",
    "- the confusing way the author described something\n",
    "- un-justified claim -- which you find hard to swallow but experts seem to accept\n",
    "- etc. et.\n",
    "\n",
    "If you have listed here some points-of-confusion, and have shown evidence that you did your share of research, study, contemplation and discussion, then the miss or mis-understanding of the corresponding part in Section 2 -- the main body of critical review, can be forgiven.\n",
    "\n",
    "Below are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Generative Model\n",
    "\n",
    "In the [paper](https://arxiv.org/abs/1406.2661)\n",
    "- What makes a \"model\" of the data? Does it mean a function mapping from observed part $x$ to some unknown target label $y$? (this is a WRONG answer here).\n",
    "- The difference between a discriminative and a generative model:\n",
    "    - Can we distinguish a generative model $G$ and a discriminative model $D$ by their inputs and outputs?\n",
    "    - Is the \"discriminator\" a discriminative model in a standard sense?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My understanding is as follows:\n",
    "\n",
    "Generative and discriminative models are defined by \"what they try to describe\": gen: $P(X)$ and $P(X|Y)$ -- building a model of how the world works, and then try to draw conclusion about whatever is concerned according to the world model. \n",
    "\n",
    "On the other hand, a discriminative model describes $P(Y|X)$ -- trying to directly answer the questions.\n",
    "\n",
    "Discriminator in GAN is different from traditional discriminative model: it describes $P(X)$, distinguishing a sample $x$ belonging to $\\mathcal{X}$ or not. While a discriminative model describes $P(Y|X)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Multilayer perceptron\n",
    "- Why people call NN multilayer perceptrons?\n",
    "- What is a single layer perceptrons?\n",
    "- What is a single perceptron?\n",
    "- How to \"train\" a perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Computational challenge in max likelihood\n",
    "- Can you show an example of a so-called maximal likelihood *problem* in generative model?\n",
    "In the following, we will consider the difficulty of this and related problems:\n",
    "- Can you think of examples, in which computing $P(X)$ is NOT difficult?\n",
    "- Discuss \"generative\" models. The name \"generative\" usually mean that the conditional relation of the \"cause\" variable $Y$ and the observed phenomenon $X$ is modelled. But there are at least 2 ways of *describe* $X|Y$\n",
    "    - generating generative model\n",
    "    - discriminating generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Read and Write Critical Review\n",
    "\n",
    "Please complete this part following the [specification](ref/spec.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Try the algorithm\n",
    "\n",
    "## 3.1 Preparation\n",
    "The following few blocks are useful book-keeping templates -- you can organise all options and other experiment configurations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a \n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Options')\n",
    "parser.add_argument(\n",
    "    '--random-seed', metavar='RS', type=int,\n",
    "    help='Random seed')\n",
    "parser.add_argument(\n",
    "    '--noise-dim', metavar='dz', type=int,\n",
    "    help='Dimension of noises z')\n",
    "parser.add_argument(\n",
    "    '--data-root', metavar='DDIR', type=str,\n",
    "    help='Training data dir; for torchvision')\n",
    "parser.add_argument(\n",
    "    '--basic-channels', metavar='ngf', type=int,\n",
    "    help='First conv layer channels')\n",
    "parser.add_argument(\n",
    "    '--learning-rate', metavar='lr', type=float,\n",
    "    help='Learning rate')\n",
    "parser.add_argument(\n",
    "    '--batch-size', metavar='n', type=int,\n",
    "    help='#.samples in a minibatch')\n",
    "parser.add_argument(\n",
    "    '--max-iter', metavar='MAXITER', type=int,\n",
    "    help='training batches')\n",
    "parser.add_argument('--report_per_n_steps_loss', type=int)\n",
    "parser.add_argument('--report_per_n_steps_draw', type=int)\n",
    "HOME_DIR = os.path.expanduser('~') # this gives your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parser.parse_args([\n",
    "    '--random-seed', '42',\n",
    "    '--noise-dim', '64',\n",
    "    '--data-root', os.path.join(HOME_DIR, 'data', 'common'),\n",
    "    '--basic-channels', '64',\n",
    "    '--learning-rate', '0.001',\n",
    "    '--batch-size', '4',\n",
    "    '--max-iter', '1000000',\n",
    "    '--report_per_n_steps_loss', '50',\n",
    "    '--report_per_n_steps_draw', '100'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    npimg -= npimg.min()\n",
    "    npimg /= npimg.max()\n",
    "    if npimg.shape[0] in [3,4]:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    else:\n",
    "        plt.imshow(npimg.squeeze(), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get data\n",
    "We will use CIFAR-10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=opt.data_root, train=True,\n",
    "    download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=opt.batch_size,\n",
    "    shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=opt.data_root, train=False,\n",
    "    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=opt.batch_size,\n",
    "    shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in trainloader:\n",
    "    break\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "show(x[0])\n",
    "print(classes[y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator net\n",
    "__Ready!__ Let's import torch and make some noises. As we know know the _format_ of the data, we are ready to build a generator net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.random.manual_seed(opt.random_seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 1, opt.noise_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Nice!__ Let's try to make our generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do two samples (any thing more than one should be OK)\n",
    "# -- why one sample is NOT so OK here?\n",
    "z = torch.randn(2, opt.noise_dim, 1, 1).to(device)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try map $z$ to a (fake) image. Please check this [nice animation](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) for [transposed convolution](https://pytorch.org/docs/stable/nn.html?highlight=convtranspose#torch.nn.ConvTranspose2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_convtr = nn.ConvTranspose2d(\n",
    "    in_channels=64, \n",
    "    out_channels=64, \n",
    "    kernel_size=3,\n",
    "    stride=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the effect of the temporary deconv\n",
    "tmp = tst_convtr(z)\n",
    "print(tmp.shape)\n",
    "tmp = tst_convtr(tmp)\n",
    "print(tmp.shape)\n",
    "tmp = tst_convtr(tmp)\n",
    "print(tmp.shape)\n",
    "tmp = tst_convtr(tmp)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we can stack the kernels as above to have a more or less image. -- Wait, we are still short of one pixel, right? Let's try another strategy. Check a tutorial on [DC-GAN](https://github.com/ResByte/pytorch-gan/blob/master/networks.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_convtr1 = nn.ConvTranspose2d(\n",
    "    in_channels=64, \n",
    "    out_channels=64, \n",
    "    kernel_size=4,\n",
    "    stride=2,\n",
    "    padding=1\n",
    ")\n",
    "# please check the effect of padding and stride. \n",
    "tmp = tst_convtr1(z)\n",
    "print(tmp.shape)\n",
    "tmp = tst_convtr1(tmp)\n",
    "print(tmp.shape)\n",
    "tmp = tst_convtr1(tmp)\n",
    "print(tmp.shape)\n",
    "tmp = tst_convtr1(tmp)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our generator. Note we cannot use the same conv layer many times now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSIZE = 4\n",
    "class GNet_CIFAR(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(GNet_CIFAR, self).__init__()\n",
    "        self.convtr1 = nn.ConvTranspose2d(\n",
    "            in_channels=opt.noise_dim, \n",
    "            out_channels=opt.basic_channels*4,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False)\n",
    "        self.convtr2 = nn.ConvTranspose2d(\n",
    "            opt.basic_channels*4, opt.basic_channels*2, \n",
    "            kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(opt.basic_channels*2)\n",
    "        self.convtr3 = nn.ConvTranspose2d(\n",
    "            opt.basic_channels*2, opt.basic_channels, \n",
    "            kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.convtr4 = nn.ConvTranspose2d(\n",
    "            opt.basic_channels, 3, \n",
    "            kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(opt.basic_channels*4)\n",
    "        self.bn2 = nn.BatchNorm2d(opt.basic_channels*2)\n",
    "        self.bn3 = nn.BatchNorm2d(opt.basic_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.convtr1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convtr2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convtr3(h)\n",
    "        h = self.bn3(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.convtr4(h)\n",
    "        h = F.tanh(h)\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GNet_CIFAR(opt)\n",
    "tmp = g(z)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BINGO!__ Now we can generate seemingly image data from noises!\n",
    "\n",
    "### Discriminator\n",
    "It is relatively easy, just reverse the generator net would do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSIZE = 4\n",
    "class DNet_CIFAR(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(DNet_CIFAR, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, \n",
    "            out_channels=opt.basic_channels,\n",
    "            kernel_size=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            opt.basic_channels, opt.basic_channels*2, \n",
    "            kernel_size=4)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            opt.basic_channels*2, opt.basic_channels*4, \n",
    "            kernel_size=4)\n",
    "        self.linear = nn.Linear(1024, 1)\n",
    "        \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(opt.basic_channels*2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = F.leaky_relu(h, 0.2, inplace=True)\n",
    "        h = F.max_pool2d(h, 2)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn1(h)\n",
    "        h = F.leaky_relu(h, 0.2, inplace=True)\n",
    "        h = F.max_pool2d(h, 2)\n",
    "        h = self.conv3(h)\n",
    "        h = self.linear(h.view(h.shape[0], -1))\n",
    "        h = F.sigmoid(h)\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet = DNet_CIFAR(opt)\n",
    "tmpd = dnet(tmp)\n",
    "print(tmpd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there. \n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnet = DNet_CIFAR(opt).to(device)\n",
    "gnet = GNet_CIFAR(opt).to(device)\n",
    "optim_discrim = Adam(dnet.parameters(), lr=opt.learning_rate)\n",
    "optim_gen = Adam(gnet.parameters(), lr=opt.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mix a set of true and fake samples:\n",
    "\n",
    "$x \\sim p_{data}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true, _ = next(iter(trainloader)) # we don't care labels for now\n",
    "print(x_true.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)\n",
    "x_fake = gnet(z).detach()\n",
    "print(x_fake.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check -- level 2\n",
    "Let us see that at least x_true and x_fake \"looks\" like each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi, yi = 10, 20\n",
    "\n",
    "tmp1 = x_true.numpy()[0][1][yi:yi+3, xi:xi+3]\n",
    "tmp2 = x_fake.numpy()[0][1][yi:yi+3, xi:xi+3]\n",
    "print(tmp1)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get something like:\n",
    "```\n",
    "tmp1:\n",
    "[[0.4039216  0.24705887 0.12156868]\n",
    " [0.39607847 0.32549024 0.15294123]\n",
    " [0.45882356 0.3803922  0.254902  ]]\n",
    "tmp2:\n",
    "[[ 0.0331481   0.00940669  0.05687319]\n",
    " [-0.05914152 -0.09792151 -0.017635  ]\n",
    " [-0.08244717  0.02879676  0.06257532]]\n",
    "```\n",
    "\n",
    "The scales don't match. And you may want to check data normalization, etc. (However, try other areas first!)\n",
    "\n",
    "Now everything is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = torch.cat((x_true, x_fake), dim=0)\n",
    "print(x_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.ones(opt.batch_size).to(device)\n",
    "y_fake = torch.zeros(opt.batch_size).to(device)\n",
    "y_sample = torch.cat((y_true, y_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dnet(x_sample).squeeze()\n",
    "print (pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design the loss\n",
    "\n",
    "Check the explanations\n",
    "[BCE for generator](https://stats.stackexchange.com/questions/242907/why-use-binary-cross-entropy-for-generator-in-adversarial-networks)\n",
    "\n",
    "[Understanding loss](https://ai.stackexchange.com/questions/3488/understanding-gan-loss-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_discrim.zero_grad()\n",
    "pred = dnet(x_sample).squeeze()\n",
    "loss = criterion(pred, y_sample)\n",
    "print(pred)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optim_discrim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we are ready to make the trainer for discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discrim():\n",
    "    \"\"\"\n",
    "    Since we will make a GAN class eventually, we will use some\n",
    "    global objects available, including\n",
    "    - trainloader\n",
    "    - opt\n",
    "    - gnet\n",
    "    - dnet\n",
    "    - optim_discrim\n",
    "    - criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    x_true, _ = next(iter(trainloader))\n",
    "    z = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)\n",
    "    x_fake = gnet(z).detach()\n",
    "    x_sample = torch.cat((x_true, x_fake), dim=0)\n",
    "    \n",
    "    y_true = torch.ones(opt.batch_size).to(device)\n",
    "    y_fake = torch.zeros(opt.batch_size).to(device)\n",
    "    y_sample = torch.cat((y_true, y_fake))\n",
    "    \n",
    "    pred = dnet(x_sample).squeeze()\n",
    "    print(pred)\n",
    "    optim_discrim.zero_grad()\n",
    "    loss = criterion(pred, y_sample)\n",
    "    loss.backward()\n",
    "    optim_discrim.step()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dis = train_discrim()\n",
    "print(loss_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Let's deal with the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(opt.batch_size*2, opt.noise_dim, 1, 1).to(device)\n",
    "x_fake = gnet(z) #! You cannot detach it NOW!\n",
    "gen_plausib = dnet(x_fake)\n",
    "print(gen_plausib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errrrrr.... something went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true, _ = next(iter(trainloader))\n",
    "z = torch.randn(opt.batch_size*2, opt.noise_dim, 1, 1).to(device)\n",
    "x_fake = gnet(z)\n",
    "x_sample = torch.cat((x_true, x_fake), dim=0)\n",
    "print(dnet(x_sample))\n",
    "print(dnet(x_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A werid issue\n",
    "Look above, for the same bunch of fake samples, the discriminator gave very different prediction.\n",
    "\n",
    "Presumbly, the batch-normalization have rendered within batch true and fake samples very different. Let's separate true and fake samples in two batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ones = torch.ones(opt.batch_size).to(device)\n",
    "y_zeros = torch.zeros(opt.batch_size).to(device)\n",
    "def train_discrim():\n",
    "    \"\"\"\n",
    "    Since we will make a GAN class eventually, we will use some\n",
    "    global objects available, including\n",
    "    - trainloader\n",
    "    - opt\n",
    "    - gnet\n",
    "    - dnet\n",
    "    - optim_discrim\n",
    "    - criterion\n",
    "    \"\"\"\n",
    "    optim_discrim.zero_grad()\n",
    "    \n",
    "    x_true, _ = next(iter(trainloader))\n",
    "    pred_1 = dnet(x_true).squeeze()\n",
    "    loss_dis_1 = criterion(pred_1, y_ones)\n",
    "    \n",
    "    \n",
    "    z = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)\n",
    "    x_fake = gnet(z).detach()\n",
    "    pred_2 = dnet(x_fake).squeeze()\n",
    "    loss_dis_2 = criterion(pred_2, y_zeros)\n",
    "    \n",
    "    loss = loss_dis_1 + loss_dis_2\n",
    "    loss.backward()\n",
    "    optim_discrim.step()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dis = train_discrim()\n",
    "print(loss_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(opt.batch_size*2, opt.noise_dim, 1, 1).to(device)\n",
    "x_fake = gnet(z) #! You cannot detach it NOW!\n",
    "gen_plausib = dnet(x_fake)\n",
    "print(gen_plausib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now everything looks fine, our generator worked terrible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen():\n",
    "    \"\"\"\n",
    "    Global variables\n",
    "    - optim_gen\n",
    "    - gnet\n",
    "    - dnet\n",
    "    - opt\n",
    "    \"\"\"\n",
    "    optim_gen.zero_grad()\n",
    "    z = torch.randn(opt.batch_size*2, opt.noise_dim, 1, 1).to(device)\n",
    "    x_fake = gnet(z) #! You cannot detach it NOW!\n",
    "    gen_plausib = dnet(x_fake)\n",
    "    loss_gen = (1.0 - gen_plausib).sum()\n",
    "    loss_gen.backward()\n",
    "    optim_gen.step()\n",
    "    return loss_gen, x_fake\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, x_fake = train_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "while iters<opt.max_iter:\n",
    "    loss_dis = train_discrim()\n",
    "    loss_gen, x_fake = train_gen()\n",
    "    \n",
    "    if iters % opt.report_per_n_steps_loss==0:\n",
    "        print(\"{:d} steps: Loss D: {:.3f} G: {:.3f} +:{:.3f}\".format(\n",
    "            iters, loss_dis, loss_gen, loss_dis+loss_gen\n",
    "        ))\n",
    "\n",
    "    if iters % opt.report_per_n_steps_draw==0:\n",
    "        show(x_fake[0])\n",
    "    iters+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1267c9940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(x_fake[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_fake = train_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1275d1d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(x_fake[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
